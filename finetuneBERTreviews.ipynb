{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "finetuneBERTreviews.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# Finetuning a BERT to determine the valence of Glassdoor/Indeed reviews\n",
        "# CS72 Final, 22S\n",
        "## Written by Leah Ryu and Michelle Chen\n",
        "### leah.ryu.22@dartmouth.edu and michelle.chen.22@dartmouth.edu\n",
        "\n",
        "With a bunch of review sentences which have \"labels\" of positive and negative, classified according to topic, we can fine-tune a BERT model to label reviews as negative or positive. Then, once we have a nice accuracy, we can use this model to label reviews that lack gold labels. These reviews come from the 'content' field of the Indeed reviews, which is a general body of text without a specified valence.\n",
        "\n",
        "We owe great thanks to the HW6 Jupyter notebooks and the many BERT tutorials available online, including:\n",
        "\n",
        "https://www.geeksforgeeks.org/fine-tuning-bert-model-for-sentiment-analysis/#:~:text=Google%20created%20a%20transformer%2Dbased,dataset%20would%20lead%20to%20overfitting\n",
        "\n",
        "https://colab.research.google.com/github/NielsRogge/Transformers-Tutorials/blob/master/BERT/Fine_tuning_BERT_(and_friends)_for_multi_label_text_classification.ipynb#scrollTo=sd1LiXGjZ420\n"
      ],
      "metadata": {
        "id": "ltdW3RwQ65MU"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install -q transformers datasets"
      ],
      "metadata": {
        "id": "UwgaUQXmN545"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "N0dyKVOBpk8d"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.model_selection import GridSearchCV\n",
        "from sklearn.model_selection import cross_val_score\n",
        "import transformers as ppb\n",
        "from transformers import AutoModel, BertTokenizerFast\n",
        "\n",
        "#for pytorch\n",
        "import torch\n",
        "import torch.nn as nn"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Parsing the text files\n",
        "We need our text files parsed into one large dataframe with <\\<content\\>> and <\\<valence\\>> labels so that we can fine-tune our BERT with it. Let's take all the already-labeled data from each company â€” so, everything excluding the neutral data from Indeed. We can first use this data without worrying about topic categories or dates to fine-tune the BERT. \n"
      ],
      "metadata": {
        "id": "AZTK73U3SU04"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Libraries needed to import files from drive\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive/')"
      ],
      "metadata": {
        "id": "wB7yT-ztvXjm",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "2cab19df-8c63-46c6-ac44-4ed44293fcb3"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/drive/; to attempt to forcibly remount, call drive.mount(\"/content/drive/\", force_remount=True).\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Removing trailing whitespace from sentences (unneeded newlines, single-spaces, etc.)\n",
        "def remove_whitespace_from(review_sentences):\n",
        "  stripped_sentences = []\n",
        "  for sentence in review_sentences:\n",
        "    stripped_sentence = sentence.rstrip()\n",
        "    stripped_sentences.append(stripped_sentence)\n",
        "  return stripped_sentences"
      ],
      "metadata": {
        "id": "49qwH-TP6yD3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Open all the files we need: pos and neg classification data\n",
        "# for the four companies, from Glassdoor and Indeed.\n",
        "\n",
        "# Riot reviews\n",
        "\n",
        "f1 = open(\"/content/drive/MyDrive/compling_final/Glassdoor/riotProsClassified.txt\", 'r')\n",
        "riotPos = remove_whitespace_from(f1.readlines())\n",
        "\n",
        "f2 = open(\"/content/drive/MyDrive/compling_final/Glassdoor/riotConsClassified.txt\", 'r')\n",
        "riotNeg = remove_whitespace_from(f2.readlines())\n",
        "\n",
        "f3 = open(\"/content/drive/MyDrive/compling_final/Indeed/riotProsIndeedClassified.txt\", 'r')\n",
        "riotIndeedPos = remove_whitespace_from(f3.readlines())\n",
        "\n",
        "f4 = open(\"/content/drive/MyDrive/compling_final/Indeed/riotConsIndeedClassified.txt\", 'r')\n",
        "riotIndeedNeg = remove_whitespace_from(f4.readlines())\n",
        "\n",
        "# Sony reviews\n",
        "\n",
        "f5 = open(\"/content/drive/MyDrive/compling_final/Glassdoor/sonyProsClassified.txt\", 'r')\n",
        "sonyPos = remove_whitespace_from(f5.readlines())\n",
        "\n",
        "f6 = open(\"/content/drive/MyDrive/compling_final/Glassdoor/sonyConsClassified.txt\", 'r')\n",
        "sonyNeg = remove_whitespace_from(f6.readlines())\n",
        "\n",
        "f7 = open(\"/content/drive/MyDrive/compling_final/Indeed/sonyProsIndeedClassified.txt\", 'r')\n",
        "sonyIndeedPos = remove_whitespace_from(f7.readlines())\n",
        "\n",
        "f8 = open(\"/content/drive/MyDrive/compling_final/Indeed/sonyConsIndeedClassified.txt\", 'r')\n",
        "sonyIndeedNeg = remove_whitespace_from(f8.readlines())\n",
        "\n",
        "# Ubisoft reviews\n",
        "\n",
        "f9 = open(\"/content/drive/MyDrive/compling_final/Glassdoor/ubisoftProsClassified.txt\", 'r')\n",
        "ubisoftPos = remove_whitespace_from(f9.readlines())\n",
        "\n",
        "f10 = open(\"/content/drive/MyDrive/compling_final/Glassdoor/ubisoftConsClassified.txt\", 'r')\n",
        "ubisoftNeg = remove_whitespace_from(f10.readlines())\n",
        "\n",
        "f11 = open(\"/content/drive/MyDrive/compling_final/Indeed/ubisoftProsIndeedClassified.txt\", 'r')\n",
        "ubisoftIndeedPos = remove_whitespace_from(f11.readlines())\n",
        "\n",
        "f12 = open(\"/content/drive/MyDrive/compling_final/Indeed/ubisoftConsIndeedClassified.txt\", 'r')\n",
        "ubisoftIndeedNeg = remove_whitespace_from(f12.readlines())\n",
        "\n",
        "# Activision reviews\n",
        "\n",
        "f13 = open(\"/content/drive/MyDrive/compling_final/Glassdoor/activisionProsClassified.txt\", 'r')\n",
        "activisionPos = remove_whitespace_from(f13.readlines())\n",
        "\n",
        "f14 = open(\"/content/drive/MyDrive/compling_final/Glassdoor/activisionConsClassified.txt\", 'r')\n",
        "activisionNeg = remove_whitespace_from(f14.readlines())\n",
        "\n",
        "f15 = open(\"/content/drive/MyDrive/compling_final/Indeed/activisionProsIndeedClassified.txt\", 'r')\n",
        "activisionIndeedPos = remove_whitespace_from(f15.readlines())\n",
        "\n",
        "f16 = open(\"/content/drive/MyDrive/compling_final/Indeed/activisionConsIndeedClassified.txt\", 'r')\n",
        "activisionIndeedNeg = remove_whitespace_from(f16.readlines())"
      ],
      "metadata": {
        "id": "PFO0_344b2_A"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# We need to store all the data in one big dataframe with the correct labels.\n",
        "# https://cmdlinetips.com/2018/01/how-to-create-pandas-dataframe-from-multiple-lists/\n",
        "\n",
        "# As per the tutorial above, we'll make two long lists, then put them into a \n",
        "# dictionary and use that to make the dataframe\n",
        "features = []\n",
        "labels = []\n",
        "\n",
        "# True = positive, False = negative\n",
        "def appendFilesToLabelsAndFeaturesList(valence, featuresList):\n",
        "  for i in range(len(featuresList)):\n",
        "    feature = featuresList[i].strip(\"\\n\")\n",
        "    if (feature != \"[LISTSEP]\"):\n",
        "      features.append(featuresList[i])\n",
        "      if (valence):\n",
        "        labels.append(1)\n",
        "      else:\n",
        "        labels.append(0)\n",
        "\n",
        "# Appending Riot reviews\n",
        "appendFilesToLabelsAndFeaturesList(True, riotPos)\n",
        "appendFilesToLabelsAndFeaturesList(False, riotNeg)\n",
        "appendFilesToLabelsAndFeaturesList(True, riotIndeedPos)\n",
        "appendFilesToLabelsAndFeaturesList(False, riotIndeedNeg)\n",
        "\n",
        "# Appending Sony reviews\n",
        "appendFilesToLabelsAndFeaturesList(True, sonyPos)\n",
        "appendFilesToLabelsAndFeaturesList(False, sonyNeg)\n",
        "appendFilesToLabelsAndFeaturesList(True, sonyIndeedPos)\n",
        "appendFilesToLabelsAndFeaturesList(False, sonyIndeedNeg)\n",
        "\n",
        "# Appending Ubisoft reviews\n",
        "appendFilesToLabelsAndFeaturesList(True, ubisoftPos)\n",
        "appendFilesToLabelsAndFeaturesList(False, ubisoftNeg)\n",
        "appendFilesToLabelsAndFeaturesList(True, ubisoftIndeedPos)\n",
        "appendFilesToLabelsAndFeaturesList(False, ubisoftIndeedNeg)\n",
        "\n",
        "# Appending Activision reviews\n",
        "appendFilesToLabelsAndFeaturesList(True, activisionPos)\n",
        "appendFilesToLabelsAndFeaturesList(False, activisionNeg)\n",
        "appendFilesToLabelsAndFeaturesList(True, activisionIndeedPos)\n",
        "appendFilesToLabelsAndFeaturesList(False, activisionIndeedNeg)"
      ],
      "metadata": {
        "id": "6CLGEOTpdmcr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# https://www.geeksforgeeks.org/python-shuffle-two-lists-with-same-order/\n",
        "# Python3 code to demonstrate working of shuffle two lists with same order\n",
        "# using zip() + * operator + shuffle()\n",
        "import random\n",
        "\n",
        "# Shuffle two lists with same order\n",
        "temp = list(zip(features, labels))\n",
        "random.shuffle(temp)\n",
        "\n",
        "features, labels = zip(*temp)\n",
        "# These come out as tuples and must be cast to lists\n",
        "features, labels = list(features), list(labels)"
      ],
      "metadata": {
        "id": "zj5UKC1TNkoP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "dictionary = {'features': features, 'labels': labels}\n",
        "df = pd.DataFrame(dictionary)"
      ],
      "metadata": {
        "id": "F2QpT2tLFsG9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df"
      ],
      "metadata": {
        "id": "6kHh4Wdy7Mls",
        "outputId": "fd703f63-22d6-405b-ca46-7a1d47d56982",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 424
        }
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "                                                features  labels\n",
              "0      The Oceania branch of Riot is also a little di...       1\n",
              "1      Everybody is really open and ready to listen t...       1\n",
              "2      Here in HK office , I know everybody , that â€™ ...       1\n",
              "3      â€”â€”â€” Multi Cultural office â€”â€”â€” We have people f...       1\n",
              "4      It 's a great place for someone who is a gamer...       1\n",
              "...                                                  ...     ...\n",
              "12295                          Compensation , hour , Nda       0\n",
              "12296  Overworked , under paid contract work , very l...       0\n",
              "12297                              Not being paid enough       0\n",
              "12298                           salary are not very high       0\n",
              "12299  long term temp , standard pay for temp , but b...       0\n",
              "\n",
              "[12300 rows x 2 columns]"
            ],
            "text/html": [
              "\n",
              "  <div id=\"df-16ef2b52-44a6-42ad-9fed-d743145a9c47\">\n",
              "    <div class=\"colab-df-container\">\n",
              "      <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>features</th>\n",
              "      <th>labels</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>The Oceania branch of Riot is also a little di...</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>Everybody is really open and ready to listen t...</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>Here in HK office , I know everybody , that â€™ ...</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>â€”â€”â€” Multi Cultural office â€”â€”â€” We have people f...</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>It 's a great place for someone who is a gamer...</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>...</th>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>12295</th>\n",
              "      <td>Compensation , hour , Nda</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>12296</th>\n",
              "      <td>Overworked , under paid contract work , very l...</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>12297</th>\n",
              "      <td>Not being paid enough</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>12298</th>\n",
              "      <td>salary are not very high</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>12299</th>\n",
              "      <td>long term temp , standard pay for temp , but b...</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>12300 rows Ã— 2 columns</p>\n",
              "</div>\n",
              "      <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-16ef2b52-44a6-42ad-9fed-d743145a9c47')\"\n",
              "              title=\"Convert this dataframe to an interactive table.\"\n",
              "              style=\"display:none;\">\n",
              "        \n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "       width=\"24px\">\n",
              "    <path d=\"M0 0h24v24H0V0z\" fill=\"none\"/>\n",
              "    <path d=\"M18.56 5.44l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94zm-11 1L8.5 8.5l.94-2.06 2.06-.94-2.06-.94L8.5 2.5l-.94 2.06-2.06.94zm10 10l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94z\"/><path d=\"M17.41 7.96l-1.37-1.37c-.4-.4-.92-.59-1.43-.59-.52 0-1.04.2-1.43.59L10.3 9.45l-7.72 7.72c-.78.78-.78 2.05 0 2.83L4 21.41c.39.39.9.59 1.41.59.51 0 1.02-.2 1.41-.59l7.78-7.78 2.81-2.81c.8-.78.8-2.07 0-2.86zM5.41 20L4 18.59l7.72-7.72 1.47 1.35L5.41 20z\"/>\n",
              "  </svg>\n",
              "      </button>\n",
              "      \n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      flex-wrap:wrap;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "      <script>\n",
              "        const buttonEl =\n",
              "          document.querySelector('#df-16ef2b52-44a6-42ad-9fed-d743145a9c47 button.colab-df-convert');\n",
              "        buttonEl.style.display =\n",
              "          google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "        async function convertToInteractive(key) {\n",
              "          const element = document.querySelector('#df-16ef2b52-44a6-42ad-9fed-d743145a9c47');\n",
              "          const dataTable =\n",
              "            await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                     [key], {});\n",
              "          if (!dataTable) return;\n",
              "\n",
              "          const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "            '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "            + ' to learn more about interactive tables.';\n",
              "          element.innerHTML = '';\n",
              "          dataTable['output_type'] = 'display_data';\n",
              "          await google.colab.output.renderOutput(dataTable, element);\n",
              "          const docLink = document.createElement('div');\n",
              "          docLink.innerHTML = docLinkHtml;\n",
              "          element.appendChild(docLink);\n",
              "        }\n",
              "      </script>\n",
              "    </div>\n",
              "  </div>\n",
              "  "
            ]
          },
          "metadata": {},
          "execution_count": 8
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## BERT"
      ],
      "metadata": {
        "id": "eNcHmddq7rnI"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Import BERT-base pretrained model\n",
        "# https://huggingface.co/bert-base-uncased\n",
        "bert = AutoModel.from_pretrained('bert-base-uncased')\n",
        "\n",
        "# Load the fast BERT tokenizer\n",
        "tokenizer = BertTokenizerFast.from_pretrained('bert-base-uncased')"
      ],
      "metadata": {
        "id": "0Xq_QtkkTYwG",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "c03be68f-0fba-4da1-b75d-83d447de2184"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertModel: ['cls.predictions.bias', 'cls.predictions.decoder.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.LayerNorm.weight', 'cls.seq_relationship.weight', 'cls.seq_relationship.bias', 'cls.predictions.transform.LayerNorm.bias']\n",
            "- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
            "- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Now that we have our dataframe sorted as two columns, one with features (in this case, our review sentences) and the second with labels (0 or 1 indicating negative or positive), we can go ahead and split up our data into training, validation, and testing sets."
      ],
      "metadata": {
        "id": "80sdCCXGgXVP"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# We'll use the ratio .70: .15: .15, first splitting up into 0.7 and 0.3, then \n",
        "# splitting the 0.3 in half.\n",
        "train_text, temp_text, train_labels, temp_labels = train_test_split(df['features'], df['labels'], \n",
        "                                                                    random_state=2021, \n",
        "                                                                    test_size=0.3, \n",
        "                                                                    stratify=df['labels'])\n",
        "\n",
        "\n",
        "val_text, test_text, val_labels, test_labels = train_test_split(temp_text, temp_labels, \n",
        "                                                                random_state=2021, \n",
        "                                                                test_size=0.5, \n",
        "                                                                stratify=temp_labels)"
      ],
      "metadata": {
        "id": "TaITrVcUaOFb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Now we're going to tokenize the data and encode it into a format that BERT can read. Under the hood, tokenization is the separation of sentences into their tokens (which look a lot like words but are often more granular) and the addition of the `[CLS]` and `[SEP]` tokens at the beginning and end of the sequence. Then, encoding means transforming tokens into their `input_ids`, which are integers."
      ],
      "metadata": {
        "id": "B7GEp3ZigrG_"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "tokenizedTrain = train_text.apply((lambda x: tokenizer.encode(x, add_special_tokens=True)))\n",
        "tokenizedVal = val_text.apply((lambda x: tokenizer.encode(x, add_special_tokens=True)))\n",
        "tokenizedTest = test_text.apply((lambda x: tokenizer.encode(x, add_special_tokens=True)))"
      ],
      "metadata": {
        "id": "aQPjK4vZVwNZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Now we have a number of encoded token vectors of varying lengths. We need to pad them all to the length so that we can represent all the vectors as a singular 2D array and have them processed as a batch."
      ],
      "metadata": {
        "id": "AytR6y8Fh3ED"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Given a list of token sequences, this returns the length of the longest sequence.\n",
        "def determineMaxLength(tokenized):\n",
        "  max_len = 0\n",
        "  for i in tokenized.values:\n",
        "      if len(i) > max_len:\n",
        "          max_len = len(i)\n",
        "  return max_len\n",
        "\n",
        "maxLenTrain = determineMaxLength(tokenizedTrain)\n",
        "maxLenVal = determineMaxLength(tokenizedVal)\n",
        "maxLenTest = determineMaxLength(tokenizedTest)\n",
        "\n",
        "# We'll take the longest out of all the sequences data sets and use that to determine\n",
        "# how much we should pad each sequence.\n",
        "max_len = max(maxLenTrain, maxLenVal, maxLenTest)\n",
        "\n",
        "paddedTrain = np.array([i + [0]*(max_len-len(i)) for i in tokenizedTrain.values])\n",
        "paddedVal = np.array([i + [0]*(max_len-len(i)) for i in tokenizedVal.values])\n",
        "paddedTest = np.array([i + [0]*(max_len-len(i)) for i in tokenizedTest.values])"
      ],
      "metadata": {
        "id": "O29cSy73V8Ud"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# As a sanity check, we can look at the shape of our training data array\n",
        "np.array(paddedTrain).shape"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "78KyzHK9fpuO",
        "outputId": "4f6da005-9073-456a-a705-50544175c0b6"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(8610, 222)"
            ]
          },
          "metadata": {},
          "execution_count": 13
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "features = df['features']\n",
        "labels = df['labels']"
      ],
      "metadata": {
        "id": "2NWxazRIWUfG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def labelsObjectToList(labels):\n",
        "  labelsList = []\n",
        "  for label in labels:\n",
        "    labelsList.append(int(label))\n",
        "  return labelsList"
      ],
      "metadata": {
        "id": "4FpxCitor_S8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# We convert all this tokenized data into a form that PyTorch can use.\n",
        "train_seq = torch.tensor(paddedTrain)\n",
        "train_mask = torch.tensor(np.where(paddedTrain != 0, 1, 0))\n",
        "train_y = torch.tensor(labelsObjectToList(train_labels))\n",
        "\n",
        "val_seq = torch.tensor(paddedVal)\n",
        "val_mask = torch.tensor(np.where(paddedVal != 0, 1, 0))\n",
        "val_y = torch.tensor(labelsObjectToList(val_labels))\n",
        "\n",
        "halfPaddedTest = paddedTest[:len(paddedTest)//2]\n",
        "test_seq = torch.tensor(halfPaddedTest)\n",
        "test_mask = torch.tensor(np.where(halfPaddedTest != 0, 1, 0))\n",
        "test_y = torch.tensor(labelsObjectToList(test_labels[:len(test_labels)//2]))"
      ],
      "metadata": {
        "id": "V5s4539_bR-s"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(len(test_labels[:len(test_labels)//2]))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "G3mFTYRiFDW5",
        "outputId": "e7ff3530-5c52-413a-bdd0-d6cbcb9752b7"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "922\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# IMPORTANT NOTE \n",
        "After this point, the code is DIRECTLY taken from \n",
        "https://github.com/Himabindugssn/Sentiment-classification-using-transformers with little to no modification."
      ],
      "metadata": {
        "id": "e6_S4wsasa_k"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from torch.utils.data import TensorDataset, DataLoader, RandomSampler, SequentialSampler\n",
        "\n",
        "# define a batch size\n",
        "batch_size = 64\n",
        "\n",
        "# wrap tensors\n",
        "train_data = TensorDataset(train_seq, train_mask, train_y)\n",
        "\n",
        "# sampler for sampling the data during training\n",
        "train_sampler = RandomSampler(train_data)\n",
        "\n",
        "# dataLoader for train set\n",
        "train_dataloader = DataLoader(train_data, sampler=train_sampler, batch_size=batch_size)\n",
        "\n",
        "# wrap tensors\n",
        "val_data = TensorDataset(val_seq, val_mask, val_y)\n",
        "\n",
        "# sampler for sampling the data during training\n",
        "val_sampler = SequentialSampler(val_data)\n",
        "\n",
        "# dataLoader for validation set\n",
        "val_dataloader = DataLoader(val_data, sampler = val_sampler, batch_size=batch_size)"
      ],
      "metadata": {
        "id": "CNgSbTXiscnr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# freeze the BERT architecture\n",
        "\n",
        "for param in bert.parameters():\n",
        "    param.requires_grad = False"
      ],
      "metadata": {
        "id": "NkMJElA6sjWl"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class BERT_architecture(nn.Module):\n",
        "\n",
        "    def __init__(self, bert):\n",
        "      \n",
        "      super(BERT_architecture, self).__init__()\n",
        "\n",
        "      self.bert = bert \n",
        "      \n",
        "      # dropout layer\n",
        "      self.dropout = nn.Dropout(0.2)\n",
        "      \n",
        "      # relu activation function\n",
        "      self.relu =  nn.ReLU()\n",
        "\n",
        "      # dense layer 1\n",
        "      self.fc1 = nn.Linear(768,512)\n",
        "      \n",
        "      # dense layer 2 (Output layer)\n",
        "      self.fc2 = nn.Linear(512,2)\n",
        "\n",
        "      #softmax activation function\n",
        "      self.softmax = nn.LogSoftmax(dim=1)\n",
        "\n",
        "    #define the forward pass\n",
        "    def forward(self, sent_id, mask):\n",
        "\n",
        "      #pass the inputs to the model  \n",
        "      _, cls_hs = self.bert(sent_id, attention_mask=mask, return_dict=False)\n",
        "      \n",
        "      x = self.fc1(cls_hs)\n",
        "\n",
        "      x = self.relu(x)\n",
        "\n",
        "      x = self.dropout(x)\n",
        "\n",
        "      # output layer\n",
        "      x = self.fc2(x)\n",
        "      \n",
        "      # apply softmax activation\n",
        "      x = self.softmax(x)\n",
        "\n",
        "      return x"
      ],
      "metadata": {
        "id": "SG8uCBs4so_K"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# pass the pre-trained BERT to our define architecture\n",
        "model = BERT_architecture(bert)"
      ],
      "metadata": {
        "id": "rZ4Z4BNvstkL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# optimizer from hugging face transformers\n",
        "from transformers import AdamW\n",
        "\n",
        "# define the optimizer\n",
        "optimizer = AdamW(model.parameters(),lr = 1e-5)  # learning rate"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "UjtXSBxgswLa",
        "outputId": "75dca84f-4cb2-43d7-d3ae-2978e5ebd3af"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/transformers/optimization.py:309: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
            "  FutureWarning,\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.utils.class_weight import compute_class_weight\n",
        "\n",
        "#compute the class weights\n",
        "class_weights = compute_class_weight(class_weight = \"balanced\",\n",
        "                                        classes = np.unique(train_labels),\n",
        "                                        y = train_labels \n",
        "                                     )\n",
        "print(\"class weights are {} for {}\".format(class_weights,np.unique(train_labels)))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zOGLWE3rsy77",
        "outputId": "5d5e4914-5f9b-4989-82d4-4daa120a3c18"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "class weights are [1.08849558 0.92481203] for [0 1]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#count of both the categories of training labels\n",
        "pd.value_counts(train_labels)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "nRdMvAZus1jK",
        "outputId": "2a169a84-14ca-4108-de20-36d30649f6ea"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "1    4655\n",
              "0    3955\n",
              "Name: labels, dtype: int64"
            ]
          },
          "metadata": {},
          "execution_count": 29
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#wrap class weights in tensor\n",
        "weights= torch.tensor(class_weights,dtype=torch.float)\n",
        "\n",
        "# define loss function\n",
        "# add weights to handle the \"imbalance\" in the dataset\n",
        "cross_entropy  = nn.NLLLoss(weight=weights) \n",
        "\n",
        "# number of training epochs\n",
        "epochs = 2"
      ],
      "metadata": {
        "id": "NSN6h6cvs4F1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# function to train the model\n",
        "def train():\n",
        "  \n",
        "  model.train()\n",
        "\n",
        "  total_loss, total_accuracy = 0, 0\n",
        "  \n",
        "  # empty list to save model predictions\n",
        "  total_preds=[]\n",
        "  \n",
        "  # iterate over batches\n",
        "  for step,batch in enumerate(train_dataloader):\n",
        "    \n",
        "    # progress update after every 50 batches.\n",
        "    if step % 50 == 0 and not step == 0:\n",
        "      print('  Batch {:>5,}  of  {:>5,}.'.format(step, len(train_dataloader)))\n",
        "\n",
        "    # # push the batch to gpu\n",
        "    # batch = [r.to(device) for r in batch]\n",
        " \n",
        "    sent_id, mask, labels = batch\n",
        "\n",
        "    # clear previously calculated gradients \n",
        "    model.zero_grad()        \n",
        "\n",
        "    # get model predictions for the current batch\n",
        "    preds = model(sent_id, mask)\n",
        "\n",
        "    # compute the loss between actual and predicted values\n",
        "    loss = cross_entropy(preds, labels)\n",
        "\n",
        "    # add on to the total loss\n",
        "    total_loss = total_loss + loss.item()\n",
        "\n",
        "    # backward pass to calculate the gradients\n",
        "    loss.backward()\n",
        "\n",
        "    # clip the the gradients to 1.0. It helps in preventing the exploding gradient problem\n",
        "    torch.nn.utils.clip_grad_norm_(model.parameters(), 1.0)\n",
        "\n",
        "    # update parameters\n",
        "    optimizer.step()\n",
        "\n",
        "    preds = preds.detach().numpy()\n",
        "\n",
        "    # append the model predictions\n",
        "    total_preds.append(preds)\n",
        "\n",
        "  # compute the training loss of the epoch\n",
        "  avg_loss = total_loss / len(train_dataloader)\n",
        "  \n",
        "  # predictions are in the form of (no. of batches, size of batch, no. of classes).\n",
        "  # reshape the predictions in form of (number of samples, no. of classes)\n",
        "  total_preds  = np.concatenate(total_preds, axis=0)\n",
        "\n",
        "  #returns the loss and predictions\n",
        "  return avg_loss, total_preds"
      ],
      "metadata": {
        "id": "GAxVJGdvs7zw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# function for evaluating the model\n",
        "def evaluate():\n",
        "  \n",
        "  print(\"\\nEvaluating...\")\n",
        "  \n",
        "  # deactivate dropout layers\n",
        "  model.eval()\n",
        "\n",
        "  total_loss, total_accuracy = 0, 0\n",
        "  \n",
        "  # empty list to save the model predictions\n",
        "  total_preds = []\n",
        "\n",
        "  # iterate over batches\n",
        "  for step,batch in enumerate(val_dataloader):\n",
        "    \n",
        "    # Progress update every 50 batches.\n",
        "    if step % 50 == 0 and not step == 0:\n",
        "            \n",
        "      # Report progress.\n",
        "      print('  Batch {:>5,}  of  {:>5,}.'.format(step, len(val_dataloader)))\n",
        "\n",
        "    # # push the batch to gpu\n",
        "    # batch = [t.to(device) for t in batch]\n",
        "\n",
        "    sent_id, mask, labels = batch\n",
        "\n",
        "    # deactivate autograd\n",
        "    with torch.no_grad():\n",
        "      \n",
        "      # model predictions\n",
        "      preds = model(sent_id, mask)\n",
        "\n",
        "      # compute the validation loss between actual and predicted values\n",
        "      loss = cross_entropy(preds,labels)\n",
        "\n",
        "      total_loss = total_loss + loss.item()\n",
        "\n",
        "      preds = preds.detach().numpy()\n",
        "\n",
        "      total_preds.append(preds)\n",
        "\n",
        "  # compute the validation loss of the epoch\n",
        "  avg_loss = total_loss / len(val_dataloader) \n",
        "\n",
        "  # reshape the predictions in form of (number of samples, no. of classes)\n",
        "  total_preds  = np.concatenate(total_preds, axis=0)\n",
        "\n",
        "  return avg_loss, total_preds"
      ],
      "metadata": {
        "id": "katgYKm3tCYg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# set initial loss to infinite\n",
        "best_valid_loss = float('inf')\n",
        "\n",
        "# empty lists to store training and validation loss of each epoch\n",
        "train_losses=[]\n",
        "valid_losses=[]\n",
        "\n",
        "#for each epoch\n",
        "for epoch in range(epochs):\n",
        "     \n",
        "    print('\\n Epoch {:} / {:}'.format(epoch + 1, epochs))\n",
        "    \n",
        "    #train model\n",
        "    train_loss, _ = train()\n",
        "    \n",
        "    #evaluate model\n",
        "    valid_loss, _  = evaluate()\n",
        "    \n",
        "    #save the best model\n",
        "    if valid_loss < best_valid_loss:\n",
        "        best_valid_loss = valid_loss\n",
        "        torch.save(model.state_dict(), 'saved_weights.pt')\n",
        "    \n",
        "    # append training and validation loss\n",
        "    train_losses.append(train_loss)\n",
        "    valid_losses.append(valid_loss)\n",
        "    \n",
        "    print('\\nTraining Loss: {}'.format(train_loss))\n",
        "    print('Validation Loss: {}'.format(valid_loss))"
      ],
      "metadata": {
        "id": "C85jTtbXtE8S",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "85f795bf-cab1-43d3-886f-4fc2d27d767f"
      },
      "execution_count": null,
      "outputs": [
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            " Epoch 1 / 2\n",
            "  Batch    50  of    135.\n",
            "  Batch   100  of    135.\n",
            "\n",
            "Evaluating...\n",
            "\n",
            "Training Loss: 0.6847317448368779\n",
            "Validation Loss: 0.6730107747275254\n",
            "\n",
            " Epoch 2 / 2\n",
            "  Batch    50  of    135.\n",
            "  Batch   100  of    135.\n",
            "\n",
            "Evaluating...\n",
            "\n",
            "Training Loss: 0.6693249066670736\n",
            "Validation Loss: 0.6617287130191408\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# IMPORTANT NOTE\n",
        "After this point, the user should take care to save the trained model by going to the file icon on the left sidebar of the screen and double-clicking on the file titled `saved_weights.pt`. Then, store it in a safe place in Drive."
      ],
      "metadata": {
        "id": "m4fPlFsEOP-C"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#load weights of best model\n",
        "path = 'drive/MyDrive/compling_final/saved_weights.pt'\n",
        "model.load_state_dict(torch.load(path))"
      ],
      "metadata": {
        "id": "Ebu2VokbtHWA",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "b20dc07a-6652-49cc-8133-ffe4017468e0"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<All keys matched successfully>"
            ]
          },
          "metadata": {},
          "execution_count": 30
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# get predictions for test data\n",
        "with torch.no_grad():\n",
        "  preds = model(test_seq, test_mask)\n",
        "  # preds = preds.detach().cpu().numpy()\n",
        "  preds = preds.detach().numpy()"
      ],
      "metadata": {
        "id": "aWnJrL_btJif"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.metrics import classification_report"
      ],
      "metadata": {
        "id": "FUT5z25ttOxO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "pred = np.argmax(preds, axis = 1)\n",
        "print(classification_report(test_y, pred))"
      ],
      "metadata": {
        "id": "I6ngGmhRtQ_G",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "1d247de1-364e-4b6b-8712-3cd75c8f52b4"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.61      0.40      0.48       425\n",
            "           1       0.60      0.78      0.68       497\n",
            "\n",
            "    accuracy                           0.61       922\n",
            "   macro avg       0.61      0.59      0.58       922\n",
            "weighted avg       0.61      0.61      0.59       922\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Retrieve prediction counts from classification report\n",
        "def countPredictedLabels(pred):\n",
        "  pos = 0\n",
        "  neg = 0\n",
        "  for integer in pred:\n",
        "    if integer == 0:\n",
        "      neg += 1\n",
        "    else:\n",
        "      pos += 1\n",
        "  return pos, neg\n",
        "\n",
        "posCount, negCount = countPredictedLabels(pred)\n",
        "print(posCount, negCount)"
      ],
      "metadata": {
        "id": "ac0JaW7A1WlG"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}