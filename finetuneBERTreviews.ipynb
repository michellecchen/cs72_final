{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "finetuneBERTreviews.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# Finetuning a BERT to determine the valence of Glassdoor/Indeed reviews\n",
        "# CS72 Final, 22S\n",
        "## Written by Leah Ryu and Michelle Chen\n",
        "### leah.ryu.22@dartmouth.edu and michelle.chen.22@dartmouth.edu\n",
        "\n",
        "With a bunch of review sentences which have \"labels\" of positive and negative, classified according to topic, we can fine-tune a BERT model to label reviews as negative or positive. Then, once we have a nice accuracy, we can use this model to label reviews that lack gold labels. These reviews come from the 'content' field of the Indeed reviews, which is a general body of text without a specified valence.\n",
        "\n",
        "We owe great thanks to the HW6 Jupyter notebooks and the many BERT tutorials available online, including:\n",
        "\n",
        "https://www.geeksforgeeks.org/fine-tuning-bert-model-for-sentiment-analysis/#:~:text=Google%20created%20a%20transformer%2Dbased,dataset%20would%20lead%20to%20overfitting\n",
        "\n",
        "https://colab.research.google.com/github/NielsRogge/Transformers-Tutorials/blob/master/BERT/Fine_tuning_BERT_(and_friends)_for_multi_label_text_classification.ipynb#scrollTo=sd1LiXGjZ420\n"
      ],
      "metadata": {
        "id": "ltdW3RwQ65MU"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install -q transformers datasets"
      ],
      "metadata": {
        "id": "UwgaUQXmN545",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "0170970a-9d0a-49f0-9cfd-ce073d6e953f"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[K     |████████████████████████████████| 4.2 MB 8.9 MB/s \n",
            "\u001b[K     |████████████████████████████████| 346 kB 45.4 MB/s \n",
            "\u001b[K     |████████████████████████████████| 596 kB 61.0 MB/s \n",
            "\u001b[K     |████████████████████████████████| 6.6 MB 46.8 MB/s \n",
            "\u001b[K     |████████████████████████████████| 86 kB 4.2 MB/s \n",
            "\u001b[K     |████████████████████████████████| 140 kB 53.6 MB/s \n",
            "\u001b[K     |████████████████████████████████| 212 kB 59.5 MB/s \n",
            "\u001b[K     |████████████████████████████████| 86 kB 5.6 MB/s \n",
            "\u001b[K     |████████████████████████████████| 1.1 MB 56.8 MB/s \n",
            "\u001b[K     |████████████████████████████████| 127 kB 47.7 MB/s \n",
            "\u001b[K     |████████████████████████████████| 271 kB 52.0 MB/s \n",
            "\u001b[K     |████████████████████████████████| 94 kB 3.1 MB/s \n",
            "\u001b[K     |████████████████████████████████| 144 kB 53.0 MB/s \n",
            "\u001b[K     |████████████████████████████████| 112 kB 54.6 MB/s \n",
            "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "datascience 0.10.6 requires folium==0.2.1, but you have folium 0.8.3 which is incompatible.\u001b[0m\n",
            "\u001b[?25h"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "N0dyKVOBpk8d"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.model_selection import GridSearchCV\n",
        "from sklearn.model_selection import cross_val_score\n",
        "import transformers as ppb\n",
        "from transformers import AutoModel, BertTokenizerFast\n",
        "\n",
        "#for pytorch\n",
        "import torch\n",
        "import torch.nn as nn"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Parsing the text files\n",
        "We need our text files parsed into one large dataframe with <\\<content\\>> and <\\<valence\\>> labels so that we can fine-tune our BERT with it. Let's take all the already-labeled data from each company — so, everything excluding the neutral data from Indeed. We can first use this data without worrying about topic categories or dates to fine-tune the BERT. \n"
      ],
      "metadata": {
        "id": "AZTK73U3SU04"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Libraries needed to import files from drive\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive/')"
      ],
      "metadata": {
        "id": "wB7yT-ztvXjm",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "069d6684-4ba0-4abd-b0f1-911e6776b1b5"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive/\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Removing trailing whitespace from sentences (unneeded newlines, single-spaces, etc.)\n",
        "def remove_whitespace_from(review_sentences):\n",
        "  stripped_sentences = []\n",
        "  for sentence in review_sentences:\n",
        "    stripped_sentence = sentence.rstrip()\n",
        "    stripped_sentences.append(stripped_sentence)\n",
        "  return stripped_sentences"
      ],
      "metadata": {
        "id": "49qwH-TP6yD3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Open all the files we need: pos and neg classification data\n",
        "# for the four companies, from Glassdoor and Indeed.\n",
        "\n",
        "# Riot reviews\n",
        "\n",
        "f1 = open(\"/content/drive/MyDrive/compling_final/Glassdoor/riotProsClassified.txt\", 'r')\n",
        "riotPos = remove_whitespace_from(f1.readlines())\n",
        "\n",
        "f2 = open(\"/content/drive/MyDrive/compling_final/Glassdoor/riotConsClassified.txt\", 'r')\n",
        "riotNeg = remove_whitespace_from(f2.readlines())\n",
        "\n",
        "f3 = open(\"/content/drive/MyDrive/compling_final/Indeed/riotProsIndeedClassified.txt\", 'r')\n",
        "riotIndeedPos = remove_whitespace_from(f3.readlines())\n",
        "\n",
        "f4 = open(\"/content/drive/MyDrive/compling_final/Indeed/riotConsIndeedClassified.txt\", 'r')\n",
        "riotIndeedNeg = remove_whitespace_from(f4.readlines())\n",
        "\n",
        "# Sony reviews\n",
        "\n",
        "f5 = open(\"/content/drive/MyDrive/compling_final/Glassdoor/sonyProsClassified.txt\", 'r')\n",
        "sonyPos = remove_whitespace_from(f5.readlines())\n",
        "\n",
        "f6 = open(\"/content/drive/MyDrive/compling_final/Glassdoor/sonyConsClassified.txt\", 'r')\n",
        "sonyNeg = remove_whitespace_from(f6.readlines())\n",
        "\n",
        "f7 = open(\"/content/drive/MyDrive/compling_final/Indeed/sonyProsIndeedClassified.txt\", 'r')\n",
        "sonyIndeedPos = remove_whitespace_from(f7.readlines())\n",
        "\n",
        "f8 = open(\"/content/drive/MyDrive/compling_final/Indeed/sonyConsIndeedClassified.txt\", 'r')\n",
        "sonyIndeedNeg = remove_whitespace_from(f8.readlines())\n",
        "\n",
        "# Ubisoft reviews\n",
        "\n",
        "f9 = open(\"/content/drive/MyDrive/compling_final/Glassdoor/ubisoftProsClassified.txt\", 'r')\n",
        "ubisoftPos = remove_whitespace_from(f9.readlines())\n",
        "\n",
        "f10 = open(\"/content/drive/MyDrive/compling_final/Glassdoor/ubisoftConsClassified.txt\", 'r')\n",
        "ubisoftNeg = remove_whitespace_from(f10.readlines())\n",
        "\n",
        "f11 = open(\"/content/drive/MyDrive/compling_final/Indeed/ubisoftProsIndeedClassified.txt\", 'r')\n",
        "ubisoftIndeedPos = remove_whitespace_from(f11.readlines())\n",
        "\n",
        "f12 = open(\"/content/drive/MyDrive/compling_final/Indeed/ubisoftConsIndeedClassified.txt\", 'r')\n",
        "ubisoftIndeedNeg = remove_whitespace_from(f12.readlines())\n",
        "\n",
        "# Activision reviews\n",
        "\n",
        "f13 = open(\"/content/drive/MyDrive/compling_final/Glassdoor/activisionProsClassified.txt\", 'r')\n",
        "activisionPos = remove_whitespace_from(f13.readlines())\n",
        "\n",
        "f14 = open(\"/content/drive/MyDrive/compling_final/Glassdoor/activisionConsClassified.txt\", 'r')\n",
        "activisionNeg = remove_whitespace_from(f14.readlines())\n",
        "\n",
        "f15 = open(\"/content/drive/MyDrive/compling_final/Indeed/activisionProsIndeedClassified.txt\", 'r')\n",
        "activisionIndeedPos = remove_whitespace_from(f15.readlines())\n",
        "\n",
        "f16 = open(\"/content/drive/MyDrive/compling_final/Indeed/activisionConsIndeedClassified.txt\", 'r')\n",
        "activisionIndeedNeg = remove_whitespace_from(f16.readlines())"
      ],
      "metadata": {
        "id": "PFO0_344b2_A"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# We need to store all the data in one big dataframe with the correct labels.\n",
        "# https://cmdlinetips.com/2018/01/how-to-create-pandas-dataframe-from-multiple-lists/\n",
        "\n",
        "# As per the tutorial above, we'll make two long lists, then put them into a \n",
        "# dictionary and use that to make the dataframe\n",
        "features = []\n",
        "labels = []\n",
        "\n",
        "# True = positive, False = negative\n",
        "def appendFilesToLabelsAndFeaturesList(valence, featuresList):\n",
        "  for i in range(len(featuresList)):\n",
        "    feature = featuresList[i].strip(\"\\n\")\n",
        "    if (feature != \"[LISTSEP]\"):\n",
        "      features.append(featuresList[i])\n",
        "      if (valence):\n",
        "        labels.append(1)\n",
        "      else:\n",
        "        labels.append(0)\n",
        "\n",
        "# Appending Riot reviews\n",
        "appendFilesToLabelsAndFeaturesList(True, riotPos)\n",
        "appendFilesToLabelsAndFeaturesList(False, riotNeg)\n",
        "appendFilesToLabelsAndFeaturesList(True, riotIndeedPos)\n",
        "appendFilesToLabelsAndFeaturesList(False, riotIndeedNeg)\n",
        "\n",
        "# Appending Sony reviews\n",
        "appendFilesToLabelsAndFeaturesList(True, sonyPos)\n",
        "appendFilesToLabelsAndFeaturesList(False, sonyNeg)\n",
        "appendFilesToLabelsAndFeaturesList(True, sonyIndeedPos)\n",
        "appendFilesToLabelsAndFeaturesList(False, sonyIndeedNeg)\n",
        "\n",
        "# Appending Ubisoft reviews\n",
        "appendFilesToLabelsAndFeaturesList(True, ubisoftPos)\n",
        "appendFilesToLabelsAndFeaturesList(False, ubisoftNeg)\n",
        "appendFilesToLabelsAndFeaturesList(True, ubisoftIndeedPos)\n",
        "appendFilesToLabelsAndFeaturesList(False, ubisoftIndeedNeg)\n",
        "\n",
        "# Appending Activision reviews\n",
        "appendFilesToLabelsAndFeaturesList(True, activisionPos)\n",
        "appendFilesToLabelsAndFeaturesList(False, activisionNeg)\n",
        "appendFilesToLabelsAndFeaturesList(True, activisionIndeedPos)\n",
        "appendFilesToLabelsAndFeaturesList(False, activisionIndeedNeg)"
      ],
      "metadata": {
        "id": "6CLGEOTpdmcr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# https://www.geeksforgeeks.org/python-shuffle-two-lists-with-same-order/\n",
        "# Python3 code to demonstrate working of shuffle two lists with same order\n",
        "# using zip() + * operator + shuffle()\n",
        "import random\n",
        "\n",
        "# Shuffle two lists with same order\n",
        "temp = list(zip(features, labels))\n",
        "random.shuffle(temp)\n",
        "\n",
        "features, labels = zip(*temp)\n",
        "# These come out as tuples and must be cast to lists\n",
        "features, labels = list(features), list(labels)"
      ],
      "metadata": {
        "id": "zj5UKC1TNkoP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "dictionary = {'features': features, 'labels': labels}\n",
        "df = pd.DataFrame(dictionary)"
      ],
      "metadata": {
        "id": "F2QpT2tLFsG9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df"
      ],
      "metadata": {
        "id": "6kHh4Wdy7Mls",
        "outputId": "fd703f63-22d6-405b-ca46-7a1d47d56982",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 424
        }
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "                                                features  labels\n",
              "0      The Oceania branch of Riot is also a little di...       1\n",
              "1      Everybody is really open and ready to listen t...       1\n",
              "2      Here in HK office , I know everybody , that ’ ...       1\n",
              "3      ——— Multi Cultural office ——— We have people f...       1\n",
              "4      It 's a great place for someone who is a gamer...       1\n",
              "...                                                  ...     ...\n",
              "12295                          Compensation , hour , Nda       0\n",
              "12296  Overworked , under paid contract work , very l...       0\n",
              "12297                              Not being paid enough       0\n",
              "12298                           salary are not very high       0\n",
              "12299  long term temp , standard pay for temp , but b...       0\n",
              "\n",
              "[12300 rows x 2 columns]"
            ],
            "text/html": [
              "\n",
              "  <div id=\"df-16ef2b52-44a6-42ad-9fed-d743145a9c47\">\n",
              "    <div class=\"colab-df-container\">\n",
              "      <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>features</th>\n",
              "      <th>labels</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>The Oceania branch of Riot is also a little di...</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>Everybody is really open and ready to listen t...</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>Here in HK office , I know everybody , that ’ ...</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>——— Multi Cultural office ——— We have people f...</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>It 's a great place for someone who is a gamer...</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>...</th>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>12295</th>\n",
              "      <td>Compensation , hour , Nda</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>12296</th>\n",
              "      <td>Overworked , under paid contract work , very l...</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>12297</th>\n",
              "      <td>Not being paid enough</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>12298</th>\n",
              "      <td>salary are not very high</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>12299</th>\n",
              "      <td>long term temp , standard pay for temp , but b...</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>12300 rows × 2 columns</p>\n",
              "</div>\n",
              "      <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-16ef2b52-44a6-42ad-9fed-d743145a9c47')\"\n",
              "              title=\"Convert this dataframe to an interactive table.\"\n",
              "              style=\"display:none;\">\n",
              "        \n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "       width=\"24px\">\n",
              "    <path d=\"M0 0h24v24H0V0z\" fill=\"none\"/>\n",
              "    <path d=\"M18.56 5.44l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94zm-11 1L8.5 8.5l.94-2.06 2.06-.94-2.06-.94L8.5 2.5l-.94 2.06-2.06.94zm10 10l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94z\"/><path d=\"M17.41 7.96l-1.37-1.37c-.4-.4-.92-.59-1.43-.59-.52 0-1.04.2-1.43.59L10.3 9.45l-7.72 7.72c-.78.78-.78 2.05 0 2.83L4 21.41c.39.39.9.59 1.41.59.51 0 1.02-.2 1.41-.59l7.78-7.78 2.81-2.81c.8-.78.8-2.07 0-2.86zM5.41 20L4 18.59l7.72-7.72 1.47 1.35L5.41 20z\"/>\n",
              "  </svg>\n",
              "      </button>\n",
              "      \n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      flex-wrap:wrap;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "      <script>\n",
              "        const buttonEl =\n",
              "          document.querySelector('#df-16ef2b52-44a6-42ad-9fed-d743145a9c47 button.colab-df-convert');\n",
              "        buttonEl.style.display =\n",
              "          google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "        async function convertToInteractive(key) {\n",
              "          const element = document.querySelector('#df-16ef2b52-44a6-42ad-9fed-d743145a9c47');\n",
              "          const dataTable =\n",
              "            await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                     [key], {});\n",
              "          if (!dataTable) return;\n",
              "\n",
              "          const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "            '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "            + ' to learn more about interactive tables.';\n",
              "          element.innerHTML = '';\n",
              "          dataTable['output_type'] = 'display_data';\n",
              "          await google.colab.output.renderOutput(dataTable, element);\n",
              "          const docLink = document.createElement('div');\n",
              "          docLink.innerHTML = docLinkHtml;\n",
              "          element.appendChild(docLink);\n",
              "        }\n",
              "      </script>\n",
              "    </div>\n",
              "  </div>\n",
              "  "
            ]
          },
          "metadata": {},
          "execution_count": 8
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## BERT"
      ],
      "metadata": {
        "id": "eNcHmddq7rnI"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Import BERT-base pretrained model\n",
        "# https://huggingface.co/bert-base-uncased\n",
        "bert = AutoModel.from_pretrained('bert-base-uncased')\n",
        "\n",
        "# Load the fast BERT tokenizer\n",
        "tokenizer = BertTokenizerFast.from_pretrained('bert-base-uncased')"
      ],
      "metadata": {
        "id": "0Xq_QtkkTYwG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Now that we have our dataframe sorted as two columns, one with features (in this case, our review sentences) and the second with labels (0 or 1 indicating negative or positive), we can go ahead and split up our data into training, validation, and testing sets."
      ],
      "metadata": {
        "id": "80sdCCXGgXVP"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# We'll use the ratio .70: .15: .15, first splitting up into 0.7 and 0.3, then \n",
        "# splitting the 0.3 in half.\n",
        "train_text, temp_text, train_labels, temp_labels = train_test_split(df['features'], df['labels'], \n",
        "                                                                    random_state=2021, \n",
        "                                                                    test_size=0.3, \n",
        "                                                                    stratify=df['labels'])\n",
        "\n",
        "\n",
        "val_text, test_text, val_labels, test_labels = train_test_split(temp_text, temp_labels, \n",
        "                                                                random_state=2021, \n",
        "                                                                test_size=0.5, \n",
        "                                                                stratify=temp_labels)"
      ],
      "metadata": {
        "id": "TaITrVcUaOFb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Now we're going to tokenize the data and encode it into a format that BERT can read. Under the hood, tokenization is the separation of sentences into their tokens (which look a lot like words but are often more granular) and the addition of the `[CLS]` and `[SEP]` tokens at the beginning and end of the sequence. Then, encoding means transforming tokens into their `input_ids`, which are integers."
      ],
      "metadata": {
        "id": "B7GEp3ZigrG_"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "tokenizedTrain = train_text.apply((lambda x: tokenizer.encode(x, add_special_tokens=True)))\n",
        "tokenizedVal = val_text.apply((lambda x: tokenizer.encode(x, add_special_tokens=True)))\n",
        "tokenizedTest = test_text.apply((lambda x: tokenizer.encode(x, add_special_tokens=True)))"
      ],
      "metadata": {
        "id": "aQPjK4vZVwNZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Now we have a number of encoded token vectors of varying lengths. We need to pad them all to the length so that we can represent all the vectors as a singular 2D array and have them processed as a batch."
      ],
      "metadata": {
        "id": "AytR6y8Fh3ED"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Given a list of token sequences, this returns the length of the longest sequence.\n",
        "def determineMaxLength(tokenized):\n",
        "  max_len = 0\n",
        "  for i in tokenized.values:\n",
        "      if len(i) > max_len:\n",
        "          max_len = len(i)\n",
        "  return max_len\n",
        "\n",
        "maxLenTrain = determineMaxLength(tokenizedTrain)\n",
        "maxLenVal = determineMaxLength(tokenizedVal)\n",
        "maxLenTest = determineMaxLength(tokenizedTest)\n",
        "\n",
        "# We'll take the longest out of all the sequences data sets and use that to determine\n",
        "# how much we should pad each sequence.\n",
        "max_len = max(maxLenTrain, maxLenVal, maxLenTest)\n",
        "\n",
        "paddedTrain = np.array([i + [0]*(max_len-len(i)) for i in tokenizedTrain.values])\n",
        "paddedVal = np.array([i + [0]*(max_len-len(i)) for i in tokenizedVal.values])\n",
        "paddedTest = np.array([i + [0]*(max_len-len(i)) for i in tokenizedTest.values])"
      ],
      "metadata": {
        "id": "O29cSy73V8Ud"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# As a sanity check, we can look at the shape of our training data array\n",
        "np.array(paddedTrain).shape"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "78KyzHK9fpuO",
        "outputId": "7a8359c0-1eae-4396-ec44-0be12160db0e"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(8610, 222)"
            ]
          },
          "metadata": {},
          "execution_count": 13
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "features = df['features']\n",
        "labels = df['labels']"
      ],
      "metadata": {
        "id": "2NWxazRIWUfG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def labelsObjectToList(labels):\n",
        "  labelsList = []\n",
        "  for label in labels:\n",
        "    labelsList.append(int(label))\n",
        "  return labelsList"
      ],
      "metadata": {
        "id": "4FpxCitor_S8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# We convert all this tokenized data into a form that PyTorch can use.\n",
        "train_seq = torch.tensor(paddedTrain)\n",
        "train_mask = torch.tensor(np.where(paddedTrain != 0, 1, 0))\n",
        "train_y = torch.tensor(labelsObjectToList(train_labels))\n",
        "\n",
        "val_seq = torch.tensor(paddedVal)\n",
        "val_mask = torch.tensor(np.where(paddedVal != 0, 1, 0))\n",
        "val_y = torch.tensor(labelsObjectToList(val_labels))\n",
        "\n",
        "test_seq = torch.tensor(paddedTest)\n",
        "test_mask = torch.tensor(np.where(paddedTest != 0, 1, 0))\n",
        "test_y = torch.tensor(labelsObjectToList(test_labels))"
      ],
      "metadata": {
        "id": "V5s4539_bR-s"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# IMPORTANT NOTE \n",
        "After this point, the code is DIRECTLY taken from \n",
        "https://github.com/Himabindugssn/Sentiment-classification-using-transformers with little to no modification."
      ],
      "metadata": {
        "id": "e6_S4wsasa_k"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from torch.utils.data import TensorDataset, DataLoader, RandomSampler, SequentialSampler\n",
        "\n",
        "# define a batch size\n",
        "batch_size = 64\n",
        "\n",
        "# wrap tensors\n",
        "train_data = TensorDataset(train_seq, train_mask, train_y)\n",
        "\n",
        "# sampler for sampling the data during training\n",
        "train_sampler = RandomSampler(train_data)\n",
        "\n",
        "# dataLoader for train set\n",
        "train_dataloader = DataLoader(train_data, sampler=train_sampler, batch_size=batch_size)\n",
        "\n",
        "# wrap tensors\n",
        "val_data = TensorDataset(val_seq, val_mask, val_y)\n",
        "\n",
        "# sampler for sampling the data during training\n",
        "val_sampler = SequentialSampler(val_data)\n",
        "\n",
        "# dataLoader for validation set\n",
        "val_dataloader = DataLoader(val_data, sampler = val_sampler, batch_size=batch_size)"
      ],
      "metadata": {
        "id": "CNgSbTXiscnr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# freeze the BERT architecture\n",
        "\n",
        "for param in bert.parameters():\n",
        "    param.requires_grad = False"
      ],
      "metadata": {
        "id": "NkMJElA6sjWl"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class BERT_architecture(nn.Module):\n",
        "\n",
        "    def __init__(self, bert):\n",
        "      \n",
        "      super(BERT_architecture, self).__init__()\n",
        "\n",
        "      self.bert = bert \n",
        "      \n",
        "      # dropout layer\n",
        "      self.dropout = nn.Dropout(0.2)\n",
        "      \n",
        "      # relu activation function\n",
        "      self.relu =  nn.ReLU()\n",
        "\n",
        "      # dense layer 1\n",
        "      self.fc1 = nn.Linear(768,512)\n",
        "      \n",
        "      # dense layer 2 (Output layer)\n",
        "      self.fc2 = nn.Linear(512,2)\n",
        "\n",
        "      #softmax activation function\n",
        "      self.softmax = nn.LogSoftmax(dim=1)\n",
        "\n",
        "    #define the forward pass\n",
        "    def forward(self, sent_id, mask):\n",
        "\n",
        "      #pass the inputs to the model  \n",
        "      outputs = self.bert(sent_id, attention_mask=mask, return_dict=False)\n",
        "      print(outputs[1].size())\n",
        "      \n",
        "      x = self.fc1(outputs[1])\n",
        "\n",
        "      x = self.relu(x)\n",
        "\n",
        "      x = self.dropout(x)\n",
        "\n",
        "      # output layer\n",
        "      x = self.fc2(x)\n",
        "      \n",
        "      # apply softmax activation\n",
        "      x = self.softmax(x)\n",
        "\n",
        "      return x"
      ],
      "metadata": {
        "id": "SG8uCBs4so_K"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# pass the pre-trained BERT to our define architecture\n",
        "model = BERT_architecture(bert)"
      ],
      "metadata": {
        "id": "rZ4Z4BNvstkL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# optimizer from hugging face transformers\n",
        "from transformers import AdamW\n",
        "\n",
        "# define the optimizer\n",
        "optimizer = AdamW(model.parameters(),lr = 1e-5)  # learning rate"
      ],
      "metadata": {
        "id": "UjtXSBxgswLa"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.utils.class_weight import compute_class_weight\n",
        "\n",
        "#compute the class weights\n",
        "class_weights = compute_class_weight(class_weight = \"balanced\",\n",
        "                                        classes = np.unique(train_labels),\n",
        "                                        y = train_labels \n",
        "                                     )\n",
        "print(\"class weights are {} for {}\".format(class_weights,np.unique(train_labels)))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zOGLWE3rsy77",
        "outputId": "240b220f-ea55-4a55-f179-43b06468588d"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "class weights are [1.08849558 0.92481203] for [0 1]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#count of both the categories of training labels\n",
        "pd.value_counts(train_labels)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "nRdMvAZus1jK",
        "outputId": "22dd77fa-f7fb-4092-db79-fd1bd89988f5"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "1    4655\n",
              "0    3955\n",
              "Name: labels, dtype: int64"
            ]
          },
          "metadata": {},
          "execution_count": 24
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#wrap class weights in tensor\n",
        "weights= torch.tensor(class_weights,dtype=torch.float)\n",
        "\n",
        "# define loss function\n",
        "# add weights to handle the \"imbalance\" in the dataset\n",
        "cross_entropy  = nn.NLLLoss(weight=weights) \n",
        "\n",
        "# number of training epochs\n",
        "epochs = 12"
      ],
      "metadata": {
        "id": "NSN6h6cvs4F1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# function to train the model\n",
        "def train():\n",
        "  \n",
        "  model.train()\n",
        "\n",
        "  total_loss, total_accuracy = 0, 0\n",
        "  \n",
        "  # empty list to save model predictions\n",
        "  total_preds=[]\n",
        "  \n",
        "  # iterate over batches\n",
        "  for step,batch in enumerate(train_dataloader):\n",
        "    \n",
        "    # progress update after every 50 batches.\n",
        "    if step % 50 == 0 and not step == 0:\n",
        "      print('  Batch {:>5,}  of  {:>5,}.'.format(step, len(train_dataloader)))\n",
        "\n",
        "    # # push the batch to gpu\n",
        "    # batch = [r.to(device) for r in batch]\n",
        " \n",
        "    sent_id, mask, labels = batch\n",
        "\n",
        "    # clear previously calculated gradients \n",
        "    model.zero_grad()        \n",
        "\n",
        "    # get model predictions for the current batch\n",
        "    preds = model(sent_id, mask)\n",
        "\n",
        "    # compute the loss between actual and predicted values\n",
        "    loss = cross_entropy(preds, labels)\n",
        "\n",
        "    # add on to the total loss\n",
        "    total_loss = total_loss + loss.item()\n",
        "\n",
        "    # backward pass to calculate the gradients\n",
        "    loss.backward()\n",
        "\n",
        "    # clip the the gradients to 1.0. It helps in preventing the exploding gradient problem\n",
        "    torch.nn.utils.clip_grad_norm_(model.parameters(), 1.0)\n",
        "\n",
        "    # update parameters\n",
        "    optimizer.step()\n",
        "\n",
        "    preds = preds.detach().numpy()\n",
        "\n",
        "    # append the model predictions\n",
        "    total_preds.append(preds)\n",
        "\n",
        "  # compute the training loss of the epoch\n",
        "  avg_loss = total_loss / len(train_dataloader)\n",
        "  \n",
        "  # predictions are in the form of (no. of batches, size of batch, no. of classes).\n",
        "  # reshape the predictions in form of (number of samples, no. of classes)\n",
        "  total_preds  = np.concatenate(total_preds, axis=0)\n",
        "\n",
        "  #returns the loss and predictions\n",
        "  return avg_loss, total_preds"
      ],
      "metadata": {
        "id": "GAxVJGdvs7zw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# function for evaluating the model\n",
        "def evaluate():\n",
        "  \n",
        "  print(\"\\nEvaluating...\")\n",
        "  \n",
        "  # deactivate dropout layers\n",
        "  model.eval()\n",
        "\n",
        "  total_loss, total_accuracy = 0, 0\n",
        "  \n",
        "  # empty list to save the model predictions\n",
        "  total_preds = []\n",
        "\n",
        "  # iterate over batches\n",
        "  for step,batch in enumerate(val_dataloader):\n",
        "    \n",
        "    # Progress update every 50 batches.\n",
        "    if step % 50 == 0 and not step == 0:\n",
        "            \n",
        "      # Report progress.\n",
        "      print('  Batch {:>5,}  of  {:>5,}.'.format(step, len(val_dataloader)))\n",
        "\n",
        "    # # push the batch to gpu\n",
        "    # batch = [t.to(device) for t in batch]\n",
        "\n",
        "    sent_id, mask, labels = batch\n",
        "\n",
        "    # deactivate autograd\n",
        "    with torch.no_grad():\n",
        "      \n",
        "      # model predictions\n",
        "      preds = model(sent_id, mask)\n",
        "\n",
        "      # compute the validation loss between actual and predicted values\n",
        "      loss = cross_entropy(preds,labels)\n",
        "\n",
        "      total_loss = total_loss + loss.item()\n",
        "\n",
        "      preds = preds.detach().numpy()\n",
        "\n",
        "      total_preds.append(preds)\n",
        "\n",
        "  # compute the validation loss of the epoch\n",
        "  avg_loss = total_loss / len(val_dataloader) \n",
        "\n",
        "  # reshape the predictions in form of (number of samples, no. of classes)\n",
        "  total_preds  = np.concatenate(total_preds, axis=0)\n",
        "\n",
        "  return avg_loss, total_preds"
      ],
      "metadata": {
        "id": "katgYKm3tCYg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# set initial loss to infinite\n",
        "best_valid_loss = float('inf')\n",
        "\n",
        "# empty lists to store training and validation loss of each epoch\n",
        "train_losses=[]\n",
        "valid_losses=[]\n",
        "\n",
        "#for each epoch\n",
        "for epoch in range(epochs):\n",
        "     \n",
        "    print('\\n Epoch {:} / {:}'.format(epoch + 1, epochs))\n",
        "    \n",
        "    #train model\n",
        "    train_loss, _ = train()\n",
        "    \n",
        "    #evaluate model\n",
        "    valid_loss, _  = evaluate()\n",
        "    \n",
        "    #save the best model\n",
        "    if valid_loss < best_valid_loss:\n",
        "        best_valid_loss = valid_loss\n",
        "        torch.save(model.state_dict(), 'saved_weights.pt')\n",
        "    \n",
        "    # append training and validation loss\n",
        "    train_losses.append(train_loss)\n",
        "    valid_losses.append(valid_loss)\n",
        "    \n",
        "    print('\\nTraining Loss: {}'.format(train_loss))\n",
        "    print('Validation Loss: {}'.format(valid_loss))"
      ],
      "metadata": {
        "id": "C85jTtbXtE8S"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# IMPORTANT NOTE\n",
        "After this point, the user should take care to save the trained model by going to the file icon on the left sidebar of the screen and double-clicking on the file titled `saved_weights.pt`. Then, store it in a safe place in Drive."
      ],
      "metadata": {
        "id": "m4fPlFsEOP-C"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#load weights of best model\n",
        "path = 'drive/MyDrive/compling_final/saved_weights.pt'\n",
        "model.load_state_dict(torch.load(path))"
      ],
      "metadata": {
        "id": "Ebu2VokbtHWA",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "b20dc07a-6652-49cc-8133-ffe4017468e0"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<All keys matched successfully>"
            ]
          },
          "metadata": {},
          "execution_count": 30
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# get predictions for test data\n",
        "with torch.no_grad():\n",
        "  preds = model(test_seq, test_mask)\n",
        "  # preds = preds.detach().cpu().numpy()\n",
        "  preds = preds.detach().numpy()"
      ],
      "metadata": {
        "id": "aWnJrL_btJif"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.metrics import classification_report"
      ],
      "metadata": {
        "id": "FUT5z25ttOxO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "pred = np.argmax(preds, axis = 1)\n",
        "print(classification_report(test_y, pred))"
      ],
      "metadata": {
        "id": "I6ngGmhRtQ_G",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "1d247de1-364e-4b6b-8712-3cd75c8f52b4"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.61      0.40      0.48       425\n",
            "           1       0.60      0.78      0.68       497\n",
            "\n",
            "    accuracy                           0.61       922\n",
            "   macro avg       0.61      0.59      0.58       922\n",
            "weighted avg       0.61      0.61      0.59       922\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Using the BERT to determine the valence of neutral/unlabeled Indeed data\n",
        "\n",
        "<sub>From this point onward, the code is once again written by us.</sub>\n",
        "\n",
        "Now that we've trained our BERT and are satisfied with its performance, we'll use it to classify our unlabeled data from the Indeed general review corps. \n",
        "\n",
        "Since the output we're using is simply a series of predicted labels with no indication of review date or the sentences that were labeled as such, we need to pre-separate our data into bins corresponding to date ranges and topics to be run through the BERT. We'll try to automate this process as much as possible, then write bin pos/neg counts to files for each company for each topic. \n"
      ],
      "metadata": {
        "id": "KzoL8Ee2ulHB"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# get predictions for test data\n",
        "def predictionsForDataset(test_seq, test_mask):\n",
        "  with torch.no_grad():\n",
        "    preds = model(test_seq, test_mask)\n",
        "    preds = preds.detach().numpy()\n",
        "\n",
        "  return np.argmax(preds, axis = 1)"
      ],
      "metadata": {
        "id": "RohHnGCDwubR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Retrieve prediction counts from classification report\n",
        "def countPredictedLabels(pred):\n",
        "  pos = 0\n",
        "  neg = 0\n",
        "  for integer in pred:\n",
        "    if integer == 0:\n",
        "      neg += 1\n",
        "    else:\n",
        "      pos += 1\n",
        "  return pos, neg"
      ],
      "metadata": {
        "id": "ac0JaW7A1WlG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Create a sequence and an attention mask for a list of features\n",
        "def createSequenceAndMask(features):\n",
        "  tokenizedFeatures = features.apply((lambda x: tokenizer.encode(x, add_special_tokens=True)))\n",
        "\n",
        "  max_len = determineMaxLength(tokenizedFeatures)\n",
        "  padded = np.array([i + [0]*(max_len-len(i)) for i in tokenizedFeatures.values])\n",
        "\n",
        "  seq = torch.tensor(padded)\n",
        "  mask = torch.tensor(np.where(padded != 0, 1, 0))\n",
        "\n",
        "  return seq, mask"
      ],
      "metadata": {
        "id": "voWWy2RAysRJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import datetime\n",
        "\n",
        "# https://stackoverflow.com/questions/35147063/python-sort-lists-from-1-list-based-on-date-elements\n",
        "def zipAndSortByDate(datesList, sentsList):\n",
        "  if len(datesList) > 0:\n",
        "    temp1, temp2 = zip(*sorted(zip(datesList, sentsList), key=lambda x: datetime.datetime.strptime(x[0], \"%m/%d/%Y\")))\n",
        "    temp1, temp2 = list(temp1), list(temp2)\n",
        "    return temp1, temp2\n",
        "  else:\n",
        "    return [], []"
      ],
      "metadata": {
        "id": "DY5ClO0S1ek5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Output: one file, separated into six sections (one for each topic) using the token [LISTSEP]\n",
        "def writeMonthlyValencesToFile(allSentences, allDates, companyName):\n",
        "  filename = '/content/drive/MyDrive/compling_final/' + companyName + 'IndeedValences.txt'\n",
        "  f1 = open(filename, 'w')\n",
        "\n",
        "  # Six topics\n",
        "  sortedDates = [[],[],[],[],[],[]]\n",
        "  sortedSentences = [[],[],[],[],[],[]]\n",
        "\n",
        "  # Start on first topic list\n",
        "  listIndex = 0\n",
        "  for i in range(len(allSentences)):\n",
        "    if allSentences[i] == \"[LISTSEP]\\n\" or allSentences[i] == \"[LISTSEP]\":\n",
        "      listIndex += 1\n",
        "      continue\n",
        "    else:\n",
        "      # add the sentence and date to the appropriate list in the large list\n",
        "      sortedDates[listIndex].append(allDates[i])\n",
        "      sortedSentences[listIndex].append(allSentences[i])\n",
        "    \n",
        "  \n",
        "  # Sort every list inside the list of lists according to date.\n",
        "  # Then, write that into monthly bins in a file, with [LISTSEP] separating topics.\n",
        "  for i in range(6):\n",
        "    sortedDates[i], sortedSentences[i] = zipAndSortByDate(sortedDates[i], sortedSentences[i]) \n",
        "    \n",
        "    dts = sortedDates[i]\n",
        "    snts = sortedSentences[i]\n",
        "\n",
        "    # For each month bin, pass the month's features through the BERT. Count up\n",
        "    # the number of positives and negatives and write it into a line in the file.\n",
        "    currentMonthFeatures = []\n",
        "\n",
        "    # Only do this if there are actually dates in the topic.\n",
        "    if len(dts) > 0:\n",
        "\n",
        "      # Establish the first bin with the first year/month in the list.\n",
        "      currentMonth = dts[0].split(\"/\")[0]\n",
        "      currentYear = dts[0].split(\"/\")[2]\n",
        "      currentMonthFeatures.append(snts[0])\n",
        "\n",
        "      # Go through each date in dates for this topic, putting the counts into one\n",
        "      # bin per month. This means we'll establish and run some data through BERT\n",
        "      # every time we hit a new month bin.\n",
        "      for index in range(len(dts)):\n",
        "        dateSplit = dts[index].split(\"/\")\n",
        "        month = dateSplit[0]\n",
        "        year = dateSplit[2]\n",
        "\n",
        "        feature = snts[index]\n",
        "\n",
        "        if month == currentMonth and year == currentYear:\n",
        "          currentMonthFeatures.append(feature)\n",
        "        else:\n",
        "          # The month changed, so write to the file\n",
        "          lineString = currentMonth + \"/\" + currentYear + \":\" \n",
        "          f1.write(lineString)\n",
        "          posCount, negCount = countPredictedLabels(predictionsForDataset(createSequenceAndMask(currentMonthFeatures)))\n",
        "          f1.write(str(posCount))\n",
        "          f1.write(\"|\")\n",
        "          f1.write(str(negCount))\n",
        "          f1.write(\"\\n\")\n",
        "          currentMonth = month\n",
        "          currentYear = year\n",
        "          currentMonthFeatures = []\n",
        "          currentMonthFeatures.append(feature)\n",
        "\n",
        "    # Next topic for this company.\n",
        "    f1.write(\"[LISTSEP]\\n\")\n"
      ],
      "metadata": {
        "id": "9PY7a6Sk02DX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Libraries needed to import files from drive: run if you haven't up above \n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive/')"
      ],
      "metadata": {
        "id": "tkQih_wZSnk5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Small test: do not run\n",
        "\n",
        "# sentences = [\"place111\", \"place112\", \"place113\", \"place114\", \"place115\", \"[LISTSEP]\", \"[LISTSEP]\", \"place5\", \"place6\", \"[LISTSEP]\", \"place7\", \"place8\", \"[LISTSEP]\", \"place9\", \"place10\", \"[LISTSEP]\", \"place11\", \"place12\", \"[LISTSEP]\"]\n",
        "# dates = [\"05/06/2015\", \"05/04/2015\", \"03/02/2006\", \"07/08/2006\", \"09/15/2021\", \"[LISTSEP]\", \"[LISTSEP]\", \"04/03/2016\", \"02/02/2012\", \"[LISTSEP]\", \"04/03/2016\", \"02/02/2012\", \"[LISTSEP]\", \"04/03/2016\", \"02/02/2012\", \"[LISTSEP]\", \"04/03/2016\", \"02/02/2012\", \"[LISTSEP]\"]\n",
        "# writeMonthlyValencesToFile(sentences, dates, \"riot\")"
      ],
      "metadata": {
        "id": "RNJQ6LNtFu4U"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}